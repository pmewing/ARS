Building DAG of jobs...
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	count_reads
	1

rule count_reads:
    input: /home/joshl/minknow_data/demultiplex_dual/
    output: /home/joshl/Desktop/output/
    jobid: 0

Error in rule count_reads:
    jobid: 0
    output: /home/joshl/Desktop/output/

RuleException:
WorkflowError in line 7 of /home/joshl/PycharmProjects/snakemake/Snakefile:
URLError: <urlopen error [Errno 2] No such file or directory: '/home/joshl/PycharmProjects/snakemake/script/CountReads.py'>
  File "/home/joshl/PycharmProjects/snakemake/Snakefile", line 7, in __rule_count_reads
  File "/usr/lib/python3.6/concurrent/futures/thread.py", line 56, in run
Removing output files of failed job count_reads since they might be corrupted:
/home/joshl/Desktop/output/
Will exit after finishing currently running jobs.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2020-06-02T123008.451617.snakemake.log
